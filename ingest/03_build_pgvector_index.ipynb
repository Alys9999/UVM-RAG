{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e9b097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.13.5 (tags/v3.13.5:6cb20a2, Jun 11 2025, 16:15:46) [MSC v.1943 64 bit (AMD64)]\n",
      "PyTorch Version: 2.7.1+cu118\n",
      "CUDA Version (PyTorch): 11.8\n",
      "CUDA Available: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# import sys\n",
    "# import torch\n",
    "\n",
    "# print(\"Python Version:\", sys.version)\n",
    "# print(\"PyTorch Version:\", torch.__version__)\n",
    "# print(\"CUDA Version (PyTorch):\", torch.version.cuda)\n",
    "# print(\"CUDA Available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c5fed85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using device: cuda\n",
      "GPU name: NVIDIA GeForce RTX 3080 Ti\n",
      "WORK_DIR: c:\\Users\\41v1r\\NEU\\NLP\\UVM-RAG\\work\n",
      "TEXT_DOCS_JSONL: c:\\Users\\41v1r\\NEU\\NLP\\UVM-RAG\\work\\json_out\\uvm_text_chunks.haystack.jsonl\n",
      "PG_CONN_STR: postgresql://postgres:postgres@localhost:5432/postgres\n",
      "Embedding model: Qwen/Qwen3-Embedding-0.6B with dimension 1024\n",
      "Pgvector table: uvm_vert_docs\n"
     ]
    }
   ],
   "source": [
    "# 03_build_pgvector_index.ipynb\n",
    "# GPU-only dense indexing into Pgvector with Haystack v2\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "import torch\n",
    "\n",
    "from haystack import Document, Pipeline\n",
    "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.document_stores.types import DuplicatePolicy\n",
    "from haystack_integrations.document_stores.pgvector import PgvectorDocumentStore\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 0. Enforce GPU-only execution\n",
    "# -------------------------------------------------------------------\n",
    "if not torch.cuda.is_available():\n",
    "    raise SystemExit(\"ERROR: CUDA GPU is required; CPU fallback is disabled.\")\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "print(\"CUDA is available. Using device:\", DEVICE)\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1. Paths and constants\n",
    "# -------------------------------------------------------------------\n",
    "ROOT = Path.cwd().parent\n",
    "WORK_DIR = ROOT / \"work\"\n",
    "\n",
    "# Output produced by 02_xxx_chunking: Haystack Document JSONL\n",
    "TEXT_DOCS_JSONL = WORK_DIR / \"json_out\" / \"uvm_text_chunks.haystack.jsonl\"\n",
    "\n",
    "# Pgvector connection string; override if you prefer environment-only configuration\n",
    "PG_CONN_STR = os.environ.get(\n",
    "    \"PG_CONN_STR\",\n",
    "    \"postgresql://postgres:postgres@localhost:5432/postgres\",\n",
    ")\n",
    "os.environ[\"PG_CONN_STR\"] = PG_CONN_STR\n",
    "\n",
    "# Embedding model and dimension (must match the embedding model you choose)\n",
    "EMBED_MODEL_NAME = \"Qwen/Qwen3-Embedding-0.6B\"\n",
    "EMBED_DIM = 1024\n",
    "\n",
    "# Name of the pgvector table for this corpus\n",
    "PG_TABLE_NAME = \"uvm_vert_docs\"\n",
    "\n",
    "# Indexing batch size (number of Documents per embedding call)\n",
    "INDEX_BATCH_SIZE = 256\n",
    "\n",
    "print(\"WORK_DIR:\", WORK_DIR)\n",
    "print(\"TEXT_DOCS_JSONL:\", TEXT_DOCS_JSONL)\n",
    "print(\"PG_CONN_STR:\", PG_CONN_STR)\n",
    "print(\"Embedding model:\", EMBED_MODEL_NAME, \"with dimension\", EMBED_DIM)\n",
    "print(\"Pgvector table:\", PG_TABLE_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f0495c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8168 Documents from c:\\Users\\41v1r\\NEU\\NLP\\UVM-RAG\\work\\json_out\\uvm_text_chunks.haystack.jsonl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8168"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Helper to load Documents from the JSONL produced by step 02\n",
    "\n",
    "def load_documents_from_jsonl(path: Path) -> list[Document]:\n",
    "    if not path.is_file():\n",
    "        raise FileNotFoundError(f\"JSONL file not found: {path}\")\n",
    "\n",
    "    docs: list[Document] = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line_no, line in enumerate(f, start=1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "            except json.JSONDecodeError as e:\n",
    "                raise ValueError(f\"Invalid JSON at line {line_no}: {e}\") from e\n",
    "\n",
    "            # The line should be a dict compatible with Document.from_dict()\n",
    "            # (as written by step 02 via Document.to_dict(flatten=False))\n",
    "            doc = Document.from_dict(data)\n",
    "            docs.append(doc)\n",
    "\n",
    "    print(f\"Loaded {len(docs)} Documents from {path}\")\n",
    "    return docs\n",
    "\n",
    "\n",
    "all_docs = load_documents_from_jsonl(TEXT_DOCS_JSONL)\n",
    "len(all_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38afa7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql://postgres:postgres@localhost:5432/postgres\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getenv(\"PG_CONN_STR\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86940179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PgvectorDocumentStore initialized.\n",
      "Current document count: 0\n"
     ]
    }
   ],
   "source": [
    "from haystack.utils.auth import Secret\n",
    "\n",
    "# Initialize PgvectorDocumentStore.\n",
    "# It reads the connection string from the PG_CONN_STR environment variable.\n",
    "document_store = PgvectorDocumentStore(\n",
    "    connection_string=Secret.from_env_var(\"PG_CONN_STR\"),\n",
    "    schema_name=\"public\",\n",
    "    table_name=PG_TABLE_NAME,\n",
    "    embedding_dimension=EMBED_DIM,\n",
    "    vector_function=\"cosine_similarity\",\n",
    "    search_strategy=\"hnsw\",\n",
    "    # Set to True if you want a fresh rebuild (drops table). For safety keep False.\n",
    "    recreate_table=False,\n",
    "    hnsw_recreate_index_if_exists=False,\n",
    ")\n",
    "\n",
    "print(\"PgvectorDocumentStore initialized.\")\n",
    "print(\"Current document count:\", document_store.count_documents())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "502ec41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentenceTransformersDocumentEmbedder warmed up with model: Qwen/Qwen3-Embedding-0.6B\n",
      "DocumentWriter initialized with DuplicatePolicy.OVERWRITE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# SentenceTransformers-based document embedder (Haystack v2)\n",
    "# It uses sentence-transformers under the hood and will use GPU because CUDA is available.\n",
    "doc_embedder = SentenceTransformersDocumentEmbedder(\n",
    "    model=EMBED_MODEL_NAME,\n",
    ")\n",
    "\n",
    "# Optional but recommended to load the model weights up-front.\n",
    "doc_embedder.warm_up()\n",
    "print(\"SentenceTransformersDocumentEmbedder warmed up with model:\", EMBED_MODEL_NAME)\n",
    "\n",
    "# DocumentWriter to write into PgvectorDocumentStore.\n",
    "# We use OVERWRITE so that reruns of indexing replace existing docs with same id.\n",
    "document_writer = DocumentWriter(\n",
    "    document_store=document_store,\n",
    "    policy=DuplicatePolicy.OVERWRITE,\n",
    ")\n",
    "\n",
    "print(\"DocumentWriter initialized with DuplicatePolicy.OVERWRITE\")\n",
    "docs = doc_embedder.run(documents=[Document(content=\"test\")])[\"documents\"]\n",
    "print(len(docs[0].embedding))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22c71b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x000001E3E60FF4D0>\n",
       "ðŸš… Components\n",
       "  - embedder: SentenceTransformersDocumentEmbedder\n",
       "  - writer: DocumentWriter\n",
       "ðŸ›¤ï¸ Connections\n",
       "  - embedder.documents -> writer.documents (list[Document])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a simple indexing pipeline:\n",
    "# Documents -> SentenceTransformersDocumentEmbedder -> DocumentWriter(Pgvector)\n",
    "\n",
    "indexing_pipeline = Pipeline()\n",
    "indexing_pipeline.add_component(\"embedder\", doc_embedder)\n",
    "indexing_pipeline.add_component(\"writer\", document_writer)\n",
    "\n",
    "# The embedder outputs \"documents\" and the writer expects \"documents\" as input,\n",
    "# so we can connect components directly by name.\n",
    "indexing_pipeline.connect(\"embedder\", \"writer\")\n",
    "\n",
    "indexing_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6724982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing 8168 documents in 32 batches (batch size = 256)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:01<00:00,  4.24it/s]\n",
      "Indexing:   3%|â–Ž         | 1/32 [00:02<01:16,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 1/32] Documents written: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00,  8.43it/s]\n",
      "Indexing:   6%|â–‹         | 2/32 [00:03<00:57,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 2/32] Documents written: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00,  9.29it/s]\n",
      "Indexing:   9%|â–‰         | 3/32 [00:05<00:48,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 3/32] Documents written: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 11.47it/s]\n",
      "Indexing:  12%|â–ˆâ–Ž        | 4/32 [00:06<00:42,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 4/32] Documents written: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00,  8.57it/s]\n",
      "Indexing:  16%|â–ˆâ–Œ        | 5/32 [00:08<00:40,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 5/32] Documents written: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 12.66it/s]\n",
      "Indexing:  19%|â–ˆâ–‰        | 6/32 [00:09<00:35,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 6/32] Documents written: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00,  8.14it/s]\n",
      "Indexing:  22%|â–ˆâ–ˆâ–       | 7/32 [00:10<00:36,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 7/32] Documents written: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00,  9.69it/s]\n",
      "Indexing:  25%|â–ˆâ–ˆâ–Œ       | 8/32 [00:12<00:34,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 8/32] Documents written: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00,  9.28it/s]\n",
      "Indexing:  28%|â–ˆâ–ˆâ–Š       | 9/32 [00:13<00:32,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 9/32] Documents written: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:01<00:00,  7.87it/s]\n",
      "Indexing:  31%|â–ˆâ–ˆâ–ˆâ–      | 10/32 [00:15<00:34,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 10/32] Documents written: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 10.49it/s]\n",
      "Indexing:  34%|â–ˆâ–ˆâ–ˆâ–      | 11/32 [00:17<00:31,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 11/32] Documents written: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00,  9.07it/s]\n",
      "Indexing:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 12/32 [00:18<00:31,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 12/32] Documents written: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00,  8.26it/s]\n",
      "Indexing:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:20<00:30,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 13/32] Documents written: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00,  9.88it/s]\n",
      "Indexing:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 14/32 [00:21<00:28,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 14/32] Documents written: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00,  8.97it/s]\n",
      "Indexing:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 15/32 [00:23<00:26,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 15/32] Documents written: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00,  8.30it/s]\n",
      "Indexing:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 16/32 [00:25<00:25,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 16/32] Documents written: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 10.33it/s]\n",
      "Indexing:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 17/32 [00:26<00:23,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 17/32] Documents written: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00,  8.91it/s]\n",
      "Indexing:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 18/32 [00:28<00:22,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 18/32] Documents written: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00,  8.53it/s]\n",
      "Indexing:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 19/32 [00:29<00:20,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 19/32] Documents written: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00,  9.30it/s]\n",
      "Indexing:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 20/32 [00:31<00:18,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 20/32] Documents written: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:01<00:00,  4.00it/s]\n",
      "Indexing:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 21/32 [00:33<00:20,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 21/32] Documents written: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 10.71it/s]\n",
      "Indexing:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 22/32 [00:35<00:16,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 22/32] Documents written: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00,  9.98it/s]\n",
      "Indexing:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 23/32 [00:36<00:14,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 23/32] Documents written: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 10.22it/s]\n",
      "Indexing:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 24/32 [00:38<00:12,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 24/32] Documents written: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:01<00:00,  7.20it/s]\n",
      "Indexing:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 25/32 [00:39<00:11,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 25/32] Documents written: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:34<00:00,  4.26s/it]\n",
      "Indexing:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 26/32 [01:14<01:09, 11.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 26/32] Documents written: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:01<00:00,  5.70it/s]\n",
      "Indexing:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 27/32 [01:17<00:44,  8.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 27/32] Documents written: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:18<00:00,  9.81s/it]\n",
      "Indexing:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 28/32 [02:36<01:59, 29.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 28/32] Documents written: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [02:41<00:00, 20.16s/it]\n",
      "Indexing:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 29/32 [05:18<03:29, 69.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 29/32] Documents written: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:01<00:00,  5.02it/s]\n",
      "Indexing:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 30/32 [05:21<01:38, 49.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 30/32] Documents written: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:31<00:00,  4.00s/it]\n",
      "Indexing:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 31/32 [05:53<00:44, 44.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 31/32] Documents written: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:02<00:00,  3.23it/s]\n",
      "Indexing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [05:56<00:00, 11.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 32/32] Documents written: 232\n",
      "Indexing finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def iter_batches(seq, batch_size):\n",
    "    \"\"\"Simple batching helper.\"\"\"\n",
    "    for i in range(0, len(seq), batch_size):\n",
    "        yield seq[i : i + batch_size]\n",
    "\n",
    "\n",
    "num_docs = len(all_docs)\n",
    "num_batches = math.ceil(num_docs / INDEX_BATCH_SIZE)\n",
    "print(f\"Indexing {num_docs} documents in {num_batches} batches (batch size = {INDEX_BATCH_SIZE})\")\n",
    "\n",
    "for batch_idx, batch in enumerate(\n",
    "    tqdm(iter_batches(all_docs, INDEX_BATCH_SIZE), total=num_batches, desc=\"Indexing\"),\n",
    "    start=1,\n",
    "):\n",
    "    # Each run call embeds the batch (on GPU) and writes them into Pgvector\n",
    "    result = indexing_pipeline.run(\n",
    "        {\n",
    "            \"embedder\": {\n",
    "                \"documents\": batch,\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    written = result[\"writer\"][\"documents_written\"]\n",
    "    print(f\"[Batch {batch_idx}/{num_batches}] Documents written: {written}\")\n",
    "\n",
    "print(\"Indexing finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e0af155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents in PgvectorDocumentStore: 7847\n",
      "\n",
      "--- Sample document #1 ---\n",
      "id: 6d2968c09f90c4d09819323bccf3fa1eb53600ce7b490a9be9eb91a896c5a0b0\n",
      "content (truncated): Universal Verification Methodology (UVM) 1.2 Class Reference\n",
      "meta keys: ['std', 'uri', 'type', 'anchor', 'source', 'page_to', 'checksum', 'split_id', 'page_from', 'source_id', 'indexed_at', 'page_number', 'section_title', '_split_overlap', 'split_idx_start']\n",
      "\n",
      "--- Sample document #2 ---\n",
      "id: 81bf15ffb2198a4764e44ab04c13e4cfd2584f3a3956a814c6067efc9a46a2e2\n",
      "content (truncated): CopyrightÂ© 2011 - 2014 Accellera Systems Initiative (Accellera). All rights reserved. Accellera Systems Initiative Inc., 1370 Trancas Street #163, Napa, CA 94558, USA.\n",
      "meta keys: ['std', 'uri', 'type', 'anchor', 'source', 'page_to', 'checksum', 'split_id', 'page_from', 'source_id', 'indexed_at', 'page_number', 'section_title', '_split_overlap', 'split_idx_start']\n",
      "\n",
      "--- Sample document #3 ---\n",
      "id: 0a81e5b3bbbacf0e538fceef7b0652421e6f86c94e32c39e5d61c8abe6fad17f\n",
      "content (truncated): Accellera Systems Initiative (Accellera) Standards documents are developed within Accellera and the Technical Committees of Accellera. Accellera develops its standards through a consensus development \n",
      "meta keys: ['std', 'uri', 'type', 'anchor', 'source', 'page_to', 'checksum', 'split_id', 'page_from', 'source_id', 'indexed_at', 'page_number', 'section_title', '_split_overlap', 'split_idx_start']\n",
      "\n",
      "--- Sample document #4 ---\n",
      "id: 5e9c822268fb840fe9fe5735958f3cde105cb05107b127c19783f9f5f17f2d6c\n",
      "content (truncated): Use of an Accellera Standard is wholly voluntary. Accellera disclaims liability for any personal injury, property or other damage, of any nature whatsoever, whether special, indirect, consequential, o\n",
      "meta keys: ['std', 'uri', 'type', 'anchor', 'source', 'page_to', 'checksum', 'split_id', 'page_from', 'source_id', 'indexed_at', 'page_number', 'section_title', '_split_overlap', 'split_idx_start']\n",
      "\n",
      "--- Sample document #5 ---\n",
      "id: d5d68a6e7a73876a5ab230d72ff89d471d551ca347a63322d6618c975055ced7\n",
      "content (truncated): Accellera does not warrant or represent the accuracy or content of the material contained herein, and expressly disclaims any express or implied warranty, including any implied warranty of merchantabi\n",
      "meta keys: ['std', 'uri', 'type', 'anchor', 'source', 'page_to', 'checksum', 'split_id', 'page_from', 'source_id', 'indexed_at', 'page_number', 'section_title', '_split_overlap', 'split_idx_start']\n"
     ]
    }
   ],
   "source": [
    "# Sanity-check: total documents stored in Pgvector\n",
    "total_in_store = document_store.count_documents()\n",
    "print(\"Total documents in PgvectorDocumentStore:\", total_in_store)\n",
    "\n",
    "# Optionally inspect a few documents (without embeddings) to confirm metadata\n",
    "sample_docs = document_store.filter_documents(filters=None)[:5]\n",
    "for i, d in enumerate(sample_docs, start=1):\n",
    "    print(f\"\\n--- Sample document #{i} ---\")\n",
    "    print(\"id:\", d.id)\n",
    "    print(\"content (truncated):\", (d.content or \"\")[:200].replace(\"\\n\", \" \"))\n",
    "    print(\"meta keys:\", list(d.meta.keys()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uvm-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
