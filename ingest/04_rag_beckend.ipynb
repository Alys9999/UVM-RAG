{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bb582c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.13.5 (tags/v3.13.5:6cb20a2, Jun 11 2025, 16:15:46) [MSC v.1943 64 bit (AMD64)]\n",
      "PyTorch Version: 2.7.1+cu118\n",
      "CUDA Version (PyTorch): 11.8\n",
      "CUDA Available: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# import sys\n",
    "# import torch\n",
    "\n",
    "# print(\"Python Version:\", sys.version)\n",
    "# print(\"PyTorch Version:\", torch.__version__)\n",
    "# print(\"CUDA Version (PyTorch):\", torch.version.cuda)\n",
    "# print(\"CUDA Available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1bac71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\41v1r\\NEU\\NLP\\UVM-RAG\\uvm-rag\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 04_rag_query_hf_local.ipynb\n",
    "# Dense-only, GPU-only RAG: PGVector + SentenceTransformers + HuggingFaceLocalGenerator\n",
    "\n",
    "import os\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "import torch\n",
    "\n",
    "from haystack import Pipeline, Document\n",
    "from haystack.utils import ComponentDevice\n",
    "from haystack.components.embedders import SentenceTransformersTextEmbedder\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from haystack.components.generators import HuggingFaceLocalGenerator\n",
    "from haystack_integrations.document_stores.pgvector import PgvectorDocumentStore\n",
    "from haystack_integrations.components.retrievers.pgvector import PgvectorEmbeddingRetriever\n",
    "\n",
    "from haystack.utils import Secret\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0abebde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA detected. Using device: ComponentDevice(_single_device=Device(type=<DeviceType.GPU: 'cuda'>, id=0), _multiple_devices=None)\n",
      "GPU name: NVIDIA GeForce RTX 3080 Ti\n",
      "Postgres: postgresql://postgres:postgres@localhost:5432/postgres\n",
      "PG table: uvm_rag_docs\n",
      "Embed model: Qwen/Qwen3-Embedding-0.6B dim: 1024\n",
      "HF local model: Qwen/Qwen2.5-1.5B-Instruct task: text-generation\n"
     ]
    }
   ],
   "source": [
    "# --- 0. Enforce GPU-only for embedding and generation ---\n",
    "if not torch.cuda.is_available():\n",
    "    raise SystemExit(\"ERROR: CUDA GPU is required; CPU fallback is disabled.\")\n",
    "\n",
    "DEVICE = ComponentDevice.from_str(\"cuda:0\")\n",
    "print(\"CUDA detected. Using device:\", DEVICE)\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# --- 1. Core configuration (must be consistent with step 03) ---\n",
    "\n",
    "PG_CONN_STR = os.environ.get(\n",
    "    \"PG_CONN_STR\",\n",
    "    \"postgresql://postgres:postgres@localhost:5432/postgres\",   # adjust for your env\n",
    ")\n",
    "PG_TABLE_NAME = os.environ.get(\"UVM_RAG_PG_TABLE_NAME\", \"uvm_vert_docs\")\n",
    "\n",
    "# Embedding model (must match SentenceTransformersDocumentEmbedder in 03)\n",
    "EMBED_MODEL_NAME = os.environ.get(\"UVM_RAG_EMBED_MODEL\", \"Qwen/Qwen3-Embedding-0.6B\")\n",
    "EMBED_DIM = int(os.environ.get(\"UVM_RAG_EMBED_DIM\", \"1024\"))\n",
    "\n",
    "# Retrieval parameters\n",
    "RETRIEVER_TOP_K = int(os.environ.get(\"UVM_RAG_RETRIEVER_TOP_K\", \"20\"))\n",
    "ANSWER_TOP_K = int(os.environ.get(\"UVM_RAG_ANSWER_TOP_K\", \"8\"))\n",
    "\n",
    "# HuggingFace local generator configuration\n",
    "# This can be a HF hub id or a local path (Path object is allowed for local models).\n",
    "HF_LOCAL_MODEL = os.environ.get(\"UVM_RAG_HF_MODEL\", \"Qwen/Qwen2.5-1.5B-Instruct\")\n",
    "\n",
    "# Typical for instruction-tuned models (decoder-only); use text-generation.\n",
    "HF_TASK = os.environ.get(\"UVM_RAG_HF_TASK\", \"text-generation\")\n",
    "\n",
    "print(\"Postgres:\", PG_CONN_STR)\n",
    "print(\"PG table:\", PG_TABLE_NAME)\n",
    "print(\"Embed model:\", EMBED_MODEL_NAME, \"dim:\", EMBED_DIM)\n",
    "print(\"HF local model:\", HF_LOCAL_MODEL, \"task:\", HF_TASK)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3322d7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HNSW index already exists and won't be recreated. If you want to recreate it, pass 'hnsw_recreate_index_if_exists=True' to the Document Store constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PG_CONN_STR = postgresql://postgres:postgres@localhost:5432/postgres\n",
      "[INFO] PgvectorDocumentStore connected.\n",
      "[INFO] Current document count: 0\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv() \n",
    "print(\"PG_CONN_STR =\", os.getenv(\"PG_CONN_STR\"))\n",
    "\n",
    "document_store = PgvectorDocumentStore(\n",
    "    connection_string = Secret.from_env_var(\"PG_CONN_STR\"),\n",
    "    table_name=PG_TABLE_NAME,\n",
    "    embedding_dimension=EMBED_DIM,\n",
    "    create_extension=True,\n",
    "    \n",
    "    vector_function=\"cosine_similarity\",\n",
    "    recreate_table=False,      # do NOT drop, index is already built in 03\n",
    "    search_strategy=\"hnsw\",    # or \"exact\" if you want brute-force\n",
    "    keyword_index_name=f\"haystack_keyword_index_{PG_TABLE_NAME}\",\n",
    ")\n",
    "\n",
    "print(\"[INFO] PgvectorDocumentStore connected.\")\n",
    "print(\"[INFO] Current document count:\", document_store.count_documents())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdc9b51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PromptBuilder has 2 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] SentenceTransformersTextEmbedder warmed up on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\41v1r\\NEU\\NLP\\UVM-RAG\\uvm-rag\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\41v1r\\.cache\\huggingface\\hub\\models--Qwen--Qwen2.5-1.5B-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] HuggingFaceLocalGenerator warmed up with model: Qwen/Qwen2.5-1.5B-Instruct\n"
     ]
    }
   ],
   "source": [
    "# 3.1 GPU-based query embedder\n",
    "text_embedder = SentenceTransformersTextEmbedder(\n",
    "    model=EMBED_MODEL_NAME,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "text_embedder.warm_up()\n",
    "print(\"[INFO] SentenceTransformersTextEmbedder warmed up on GPU.\")\n",
    "\n",
    "# 3.2 Dense-only PGVector retriever\n",
    "retriever = PgvectorEmbeddingRetriever(\n",
    "    document_store=document_store,\n",
    "    top_k=RETRIEVER_TOP_K,\n",
    ")\n",
    "\n",
    "# 3.3 Citation-aware prompt template (non-chat, plain text)\n",
    "RAG_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an expert verification engineer specializing in SystemVerilog UVM.\n",
    "Use ONLY the context documents below to answer the question.\n",
    "If the answer is not clearly contained in the context, say that you do not know.\n",
    "\n",
    "Question:\n",
    "{{ query }}\n",
    "\n",
    "Context documents:\n",
    "{% for doc in documents %}\n",
    "[{{ loop.index }}]\n",
    "- std: {{ doc.meta.std | default(\"UVM-1.2\") }}\n",
    "- section_title: {{ doc.meta.section_title | default(\"N/A\") }}\n",
    "- type: {{ doc.meta.type | default(\"text\") }}\n",
    "- location: {{ doc.meta.uri | default(\"\") }}{{ doc.meta.anchor | default(\"\") }}\n",
    "\n",
    "{{ doc.content }}\n",
    "\n",
    "{% endfor %}\n",
    "\n",
    "Instructions for answering:\n",
    "- Answer concisely but precisely.\n",
    "- Use correct UVM/SystemVerilog terminology.\n",
    "- When you rely on a document, cite it as [index], for example [1], [2].\n",
    "- If multiple documents support the same statement, you can cite like [1][3].\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt_builder = PromptBuilder(template=RAG_PROMPT_TEMPLATE)\n",
    "\n",
    "# 3.4 Local Hugging Face generator on GPU\n",
    "# You may want to tune generation_kwargs for deterministic / concise answers.\n",
    "generation_kwargs = {\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"temperature\": 0.1,\n",
    "    \"do_sample\": False,\n",
    "}\n",
    "\n",
    "hf_generator = HuggingFaceLocalGenerator(\n",
    "    model=HF_LOCAL_MODEL,\n",
    "    task=HF_TASK,\n",
    "    device=DEVICE,\n",
    "    generation_kwargs=generation_kwargs,\n",
    ")\n",
    "\n",
    "hf_generator.warm_up()\n",
    "print(\"[INFO] HuggingFaceLocalGenerator warmed up with model:\", HF_LOCAL_MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51687173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] RAG pipeline topology constructed:\n",
      " - Could not draw pipeline graph: PipelineBase.draw() takes 1 positional argument but 2 were given\n"
     ]
    }
   ],
   "source": [
    "rag_pipeline = Pipeline()\n",
    "rag_pipeline.add_component(\"text_embedder\", text_embedder)\n",
    "rag_pipeline.add_component(\"retriever\", retriever)\n",
    "rag_pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
    "rag_pipeline.add_component(\"llm\", hf_generator)\n",
    "\n",
    "# Connect query embedding → retriever\n",
    "rag_pipeline.connect(\"text_embedder\", \"retriever\")\n",
    "\n",
    "# Connect retrieved docs → prompt builder\n",
    "rag_pipeline.connect(\"retriever\", \"prompt_builder.documents\")\n",
    "\n",
    "# Connect prompt → LLM\n",
    "rag_pipeline.connect(\"prompt_builder\", \"llm\")\n",
    "\n",
    "print(\"[INFO] RAG pipeline topology constructed:\")\n",
    "# If you run locally, you can render a graph:\n",
    "try:\n",
    "    rag_pipeline.draw(\"04_rag_query_hf_local_pipeline.png\")\n",
    "    print(\" - Saved pipeline graph to 04_rag_query_hf_local_pipeline.png\")\n",
    "except Exception as e:\n",
    "    print(\" - Could not draw pipeline graph:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "006feff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rag_query(\n",
    "    query: str,\n",
    "    retriever_top_k: int | None = None,\n",
    "    answer_top_k: int | None = None,\n",
    ") -> Tuple[str, List[Document], Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Run the full RAG pipeline for a single query.\n",
    "\n",
    "    - Embeds query on GPU (SentenceTransformersTextEmbedder).\n",
    "    - Retrieves dense-only neighbors from PGVectorEmbeddingRetriever.\n",
    "    - Builds a citation-aware prompt (PromptBuilder).\n",
    "    - Generates answer via HuggingFaceLocalGenerator on GPU.\n",
    "    - Returns:\n",
    "        answer_text,\n",
    "        docs_for_answer (top-N documents for UI),\n",
    "        raw_result (full pipeline output for debugging).\n",
    "    \"\"\"\n",
    "    if retriever_top_k is None:\n",
    "        retriever_top_k = RETRIEVER_TOP_K\n",
    "    if answer_top_k is None:\n",
    "        answer_top_k = ANSWER_TOP_K\n",
    "\n",
    "    # Run the whole graph once\n",
    "    result = rag_pipeline.run(\n",
    "        {\n",
    "            \"text_embedder\": {\"text\": query},\n",
    "            \"retriever\": {\"top_k\": retriever_top_k},\n",
    "            \"prompt_builder\": {\"query\": query},\n",
    "        },\n",
    "        include_outputs_from={\"retriever\", \"prompt_builder\", \"llm\"},\n",
    "    )\n",
    "\n",
    "    retrieved_docs: List[Document] = result[\"retriever\"][\"documents\"]\n",
    "    # For display and prompt length control, we keep only the first answer_top_k\n",
    "    docs_for_answer = retrieved_docs[:answer_top_k]\n",
    "\n",
    "    replies: List[str] = result[\"llm\"][\"replies\"]\n",
    "    answer_text = replies[0] if replies else \"\"\n",
    "\n",
    "    return answer_text, docs_for_answer, result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "533289cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.19s/it]\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Question ===\n",
      "In UVM 1.2, what does uvm_packer::unpack_array do when unpacking dynamic arrays?\n",
      "\n",
      "=== Answer (with [i] citations) ===\n",
      " In UVM 1.2, `uvm_packer::unpack_array` unmarshals elements from a packed array into individual variables of type T. It iterates over each element in the packed array and assigns them to corresponding variables based on their positions within the array. This function is particularly useful for handling dynamic arrays where the number of elements may vary at runtime. [1]\n",
      "\n",
      "=== Context documents (top-N used) ===\n"
     ]
    }
   ],
   "source": [
    "test_query = \"In UVM 1.2, what does uvm_packer::unpack_array do when unpacking dynamic arrays?\"\n",
    "\n",
    "answer_text, docs_used, raw = run_rag_query(\n",
    "    query=test_query,\n",
    "    retriever_top_k=15,\n",
    "    answer_top_k=8,\n",
    ")\n",
    "\n",
    "print(\"=== Question ===\")\n",
    "print(test_query)\n",
    "print(\"\\n=== Answer (with [i] citations) ===\")\n",
    "print(answer_text)\n",
    "\n",
    "print(\"\\n=== Context documents (top-N used) ===\")\n",
    "for idx, d in enumerate(docs_used, start=1):\n",
    "    uri = d.meta.get(\"uri\", \"\")\n",
    "    anchor = d.meta.get(\"anchor\", \"\")\n",
    "    section_title = d.meta.get(\"section_title\", \"N/A\")\n",
    "    doc_type = d.meta.get(\"type\", \"text\")\n",
    "    print(f\"\\n[{idx}] ({doc_type}) {section_title}\")\n",
    "    print(f\"    {uri}{anchor}\")\n",
    "    snippet = (d.content or \"\").strip().replace(\"\\n\", \" \")\n",
    "    if len(snippet) > 220:\n",
    "        snippet = snippet[:220] + \"...\"\n",
    "    print(f\"    {snippet}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uvm-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
